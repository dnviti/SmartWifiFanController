name: AI Code Analysis (ChatGPT)
on:
  pull_request:
    paths: # Specify paths to trigger the workflow on PRs
      - 'src/**.cpp'
      - 'src/**.h'
      - 'test/**.cpp'
      # Add other relevant file types and paths
  workflow_dispatch: # Allows manual triggering

jobs:
  analyze-code:
    runs-on: ubuntu-latest
    permissions:
      contents: read          # To checkout the repository
      pull-requests: write   # If you want to comment on PRs
      issues: write          # If you want to create issues

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 # Fetches all history for accurate diffs

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10' # Or your preferred Python version

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          # Install the OpenAI Python library
          pip install openai
          # If you plan to make GitHub API calls directly for issues/comments:
          # pip install requests
          # Add any other dependencies your script might need

      - name: Get changed files
        id: changed_files
        run: |
          echo "Determining files to analyze..."
          if [ "${{ github.event_name }}" == "pull_request" ]; then
            MERGE_BASE=$(git merge-base ${{ github.event.pull_request.base.sha }} HEAD)
            FILES=$(git diff --name-only $MERGE_BASE HEAD | grep -E '\.(cpp|h)$' | tr '\n' ' ')
          else # For workflow_dispatch or other events, analyze all relevant files
            FILES=$(find . -type f \( -name "*.cpp" -o -name "*.h" \) -not -path "./.git/*" -not -path "./build/*" | tr '\n' ' ')
          fi
          if [ -z "$FILES" ]; then
            echo "No relevant files found to analyze."
            echo "files_to_analyze=''" >> $GITHUB_OUTPUT
          else
            echo "Files to analyze: $FILES"
            echo "files_to_analyze=$FILES" >> $GITHUB_OUTPUT
          fi
        shell: bash

      - name: Run AI Code Analysis Script (ChatGPT)
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }} # Changed from GEMINI_API_KEY
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} 
          REPO_NAME: ${{ github.repository }}
          PR_NUMBER: ${{ github.event.pull_request.number }}
        run: |
          import os
          import time
          import random
          import json
          # Import the OpenAI library
          import openai

          # --- Configuration ---
          MAX_RETRIES = 5
          INITIAL_BACKOFF = 3  # Initial backoff delay in seconds
          INTER_FILE_DELAY = 2 # Seconds to wait between processing each file
          # Ensure this model name is correct and matches what you intend to use.
          # The logs indicated 'o4-mini', which might map to 'gpt-4o-mini'.
          # If 'gpt-4o-mini' is the correct model identifier for the API, use that.
          # Otherwise, use a model known to support temperature changes or omit it.
          OPENAI_MODEL_NAME = os.getenv('OPENAI_MODEL_OVERRIDE', 'gpt-4o-mini') # Default to gpt-4o-mini, can be overridden by env var

          OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
          if not OPENAI_API_KEY:
              print("Error: OPENAI_API_KEY not found in environment variables.")
              exit(1)

          # --- Initialize OpenAI Client ---
          try:
            client = openai.OpenAI(api_key=OPENAI_API_KEY)
            print(f"Successfully configured OpenAI client for model: {OPENAI_MODEL_NAME}")
          except Exception as e:
            print(f"Error configuring OpenAI client: {e}")
            exit(1)
          # --------------------------------

          def call_openai_api_with_backoff(file_path, file_content):
              """
              Calls the OpenAI API (Chat Completions) with exponential backoff and jitter.
              """
              print(f"Analyzing {file_path} with OpenAI model ({OPENAI_MODEL_NAME})...")
              retries = 0
              backoff_time = INITIAL_BACKOFF
              
              # --- Construct your prompt for OpenAI ---
              system_message = "You are an expert C++ code reviewer. Analyze the provided code snippet."
              user_prompt = f"Please analyze the following code from the file '{file_path}'. Identify potential bugs, suggest improvements for clarity, performance, and adherence to C++ best practices. Provide specific, actionable feedback. If there are multiple points, list them clearly.\n\n```cpp\n{file_content}\n```\n\nSuggestions:"
              # -----------------------------------------

              while retries < MAX_RETRIES:
                  try:
                      # --- OpenAI API Call (Chat Completions) ---
                      # Removed explicit temperature setting to use model's default,
                      # as 'o4-mini' (likely gpt-4o-mini) errored with temperature=0.5.
                      # If a different model is used that supports temperature, it can be added back.
                      response = client.chat.completions.create(
                          model=OPENAI_MODEL_NAME,
                          messages=[
                              {"role": "system", "content": system_message},
                              {"role": "user", "content": user_prompt}
                          ],
                          # temperature=0.5, # Removed based on error for o4-mini
                          # max_tokens=1500, # Adjust as needed
                      )
                      suggestions = response.choices[0].message.content.strip()
                      # ---------------------------------------------
                      
                      print(f"Successfully received analysis for {file_path} from OpenAI on attempt {retries + 1}.")
                      return suggestions

                  except openai.APIError as e: 
                      error_str = str(e).lower()
                      # Check for specific error related to 'temperature' for the current model
                      if e.status_code == 400 and e.body and 'param' in e.body and e.body['param'] == 'temperature' and 'unsupported_value' in e.body.get('code',''):
                          print(f"OpenAI API error for {file_path}: Model '{OPENAI_MODEL_NAME}' does not support the specified temperature or temperature setting. Error: {e.body.get('message')}")
                          print("Skipping analysis for this file due to model parameter incompatibility.")
                          return f"_Analysis skipped for this file: Model '{OPENAI_MODEL_NAME}' does not support the provided temperature setting. Error: {e.body.get('message')}_"

                      if e.status_code == 429 or "rate limit" in error_str or "too many requests" in error_str:
                          retries += 1
                          if retries >= MAX_RETRIES:
                              print(f"Warning: Max retries reached for {file_path}. Error calling OpenAI API (attempt {retries}/{MAX_RETRIES}): {e}")
                              print(f"Error: Failed to get suggestions for {file_path} after {MAX_RETRIES} attempts.")
                              return None
                          
                          retry_after = e.headers.get("retry-after")
                          if retry_after:
                              wait_time = int(retry_after) + random.uniform(0, 0.5) 
                              print(f"OpenAI API rate limit hit for {file_path} (attempt {retries}/{MAX_RETRIES}). Retrying after {wait_time:.2f} seconds (from header)... Error: {e}")
                          else:
                              wait_time = backoff_time + random.uniform(0, 1.5)
                              print(f"OpenAI API rate limit hit for {file_path} (attempt {retries}/{MAX_RETRIES}). Retrying in {wait_time:.2f} seconds... Error: {e}")
                              backoff_time *= 2
                          
                          time.sleep(wait_time)
                      elif e.status_code >= 500: 
                          retries += 1
                          if retries >= MAX_RETRIES:
                              print(f"Warning: Max retries reached for {file_path} due to server error. Error calling OpenAI API (attempt {retries}/{MAX_RETRIES}): {e}")
                              return None
                          wait_time = backoff_time + random.uniform(0, 1.5)
                          print(f"OpenAI API server error for {file_path} (attempt {retries}/{MAX_RETRIES}). Retrying in {wait_time:.2f} seconds... Error: {e}")
                          time.sleep(wait_time)
                          backoff_time *= 1.5 
                      else:
                          print(f"An unexpected OpenAI API error occurred for {file_path}: {e}")
                          if e.body: # Log more details if available
                              print(f"Error details: {e.body}")
                          return None
                  except Exception as e:
                      print(f"A general error occurred while calling OpenAI API for {file_path}: {e}")
                      retries += 1
                      if retries >= MAX_RETRIES:
                          return None
                      wait_time = backoff_time + random.uniform(0, 1.5)
                      print(f"Retrying in {wait_time:.2f} seconds...")
                      time.sleep(wait_time)
                      backoff_time *= 2
              return None

          def create_github_issue_or_comment(title, body_content, pr_number=None):
              github_token = os.getenv('GITHUB_TOKEN')
              repo_name = os.getenv('REPO_NAME')
              if not github_token or not repo_name:
                  print("Error: GITHUB_TOKEN or REPO_NAME not set. Cannot create issue/comment.")
                  print(f"Title: {title}\nBody:\n{body_content}")
                  return

              headers = {
                  "Authorization": f"token {github_token}",
                  "Accept": "application/vnd.github.v3+json"
              }
              
              api_url = f"https://api.github.com/repos/{repo_name}/issues"
              data = {"title": title, "body": body_content}
              if pr_number: 
                  data["body"] = f"AI Code Analysis for PR #{pr_number}:\n\n{body_content}"

              # --- This part requires the 'requests' library. Ensure it's installed. ---
              # import requests 
              # try:
              #     response = requests.post(api_url, json=data, headers=headers)
              #     response.raise_for_status() 
              #     print(f"Successfully created GitHub issue/comment: {title}")
              #     print(f"URL: {response.json().get('html_url')}")
              # except requests.exceptions.RequestException as e:
              #     print(f"Failed to create GitHub issue/comment '{title}'. Error: {e}")
              #     print(f"Response status: {response.status_code}, Response text: {response.text}")
              # except Exception as e:
              #     print(f"An unexpected error occurred during GitHub API call: {e}")
              # --- End of 'requests' dependent block ---
              
              print(f"--- MOCK GITHUB INTERACTION ---")
              if pr_number:
                print(f"Action: Would create issue referencing PR #{pr_number} in {repo_name}")
              else:
                print(f"Action: Would create issue in {repo_name}")
              print(f"Title: {title}")
              print(f"Body:\n{data['body']}") 
              print(f"-----------------------------")


          files_to_analyze_str = os.getenv('FILES_TO_ANALYZE', "${{ steps.changed_files.outputs.files_to_analyze }}")
          
          if not files_to_analyze_str or not files_to_analyze_str.strip():
              print("No files to analyze based on 'changed_files' output or environment variable.")
              exit(0)
          
          files_to_analyze = [f.strip() for f in files_to_analyze_str.strip().split(' ') if f.strip()]
          
          if not files_to_analyze:
              print("No valid file paths to analyze after stripping and filtering.")
              exit(0)
              
          print(f"Files queued for analysis: {files_to_analyze}")

          all_suggestions_for_report = []
          analysis_successful_count = 0

          for i, file_path in enumerate(files_to_analyze):
              print(f"\nProcessing file {i+1}/{len(files_to_analyze)}: {file_path}")
              try:
                  with open(file_path, 'r', encoding='utf-8') as f:
                      file_content = f.read()
                  
                  if not file_content.strip():
                      print(f"Skipping empty file: {file_path}")
                      all_suggestions_for_report.append(f"### Suggestions for `{file_path}`:\n\n_File is empty or contains only whitespace. Skipped._\n\n---\n")
                      continue

                  suggestions = call_openai_api_with_backoff(file_path, file_content)
                  
                  if suggestions:
                      # Check if suggestions indicate a skip due to model incompatibility
                      if "_Analysis skipped for this file" in suggestions:
                          print(f"Suggestions for {file_path} indicate a skip due to model parameter issue.")
                      else:
                          print(f"Suggestions received for {file_path}.")
                          analysis_successful_count +=1
                      all_suggestions_for_report.append(f"### Suggestions for `{file_path}`:\n\n{suggestions}\n\n---\n")
                  else:
                      print(f"No suggestions were returned by AI for {file_path} after retries.")
                      all_suggestions_for_report.append(f"### Suggestions for `{file_path}`:\n\n_No suggestions were returned by the AI after {MAX_RETRIES} attempts._\n\n---\n")

                  if len(files_to_analyze) > 1 and i < len(files_to_analyze) - 1:
                      print(f"Waiting for {INTER_FILE_DELAY} seconds before processing the next file...")
                      time.sleep(INTER_FILE_DELAY)

              except FileNotFoundError:
                  print(f"Error: File not found - {file_path}")
                  all_suggestions_for_report.append(f"### Analysis Error for `{file_path}`:\n\n_File not found during analysis._\n\n---\n")
              except Exception as e:
                  print(f"Error processing file {file_path}: {e}")
                  all_suggestions_for_report.append(f"### Analysis Error for `{file_path}`:\n\n_An unexpected error occurred: {e}_n\n---\n")

          if all_suggestions_for_report:
              pr_number_env = os.getenv('PR_NUMBER')
              report_title = "AI Code Analysis Report (OpenAI)"
              if pr_number_env:
                  report_title += f" for PR #{pr_number_env}"
              
              report_body = f"## AI Code Analysis Summary (OpenAI {OPENAI_MODEL_NAME})\n\nAnalyzed {len(files_to_analyze)} file(s). Successfully received suggestions for {analysis_successful_count} file(s).\n\n"
              report_body += "".join(all_suggestions_for_report)
              
              create_github_issue_or_comment(report_title, report_body, pr_number=pr_number_env)
          else:
              print("No suggestions were generated or errors encountered for any files that would warrant a report.")

          print("\nAI code analysis (OpenAI) process complete.")
        shell: python
